{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e623eff-f46e-4d7a-bfb2-f1ba31d6bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "from gym import envs\n",
    "import pprint\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3697f3ce-8310-4ac1-a03f-e852e9b2364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "# env = gym.make('MountainCar-v0', new_step_api=True)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0644ea-6e54-4737-9829-b8dd099bc40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space Discrete(4)\n",
      "State space Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print('Action space {}'.format(env.action_space))\n",
    "print('State space {}'.format(env.observation_space))\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dffe6517-af8d-45ed-b357-cf66cf67c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   0: [   (0.3333333333333333, 0, 0.0, False),\n",
      "           (0.3333333333333333, 0, 0.0, False),\n",
      "           (0.3333333333333333, 4, 0.0, False)],\n",
      "    1: [   (0.3333333333333333, 0, 0.0, False),\n",
      "           (0.3333333333333333, 4, 0.0, False),\n",
      "           (0.3333333333333333, 1, 0.0, False)],\n",
      "    2: [   (0.3333333333333333, 4, 0.0, False),\n",
      "           (0.3333333333333333, 1, 0.0, False),\n",
      "           (0.3333333333333333, 0, 0.0, False)],\n",
      "    3: [   (0.3333333333333333, 1, 0.0, False),\n",
      "           (0.3333333333333333, 0, 0.0, False),\n",
      "           (0.3333333333333333, 0, 0.0, False)]}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(env.P[0])\n",
    "# outputs {action: [probability (next_state), next_state, reward, done]}\n",
    "#  Left, Down, Right and Up and (corresponding to numbers 0, 1, 2 and 3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d42596a-85a6-4d0d-a285-1528fe02d411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   0: [   (0.3333333333333333, 10, 0.0, False),\n",
      "           (0.3333333333333333, 13, 0.0, False),\n",
      "           (0.3333333333333333, 14, 0.0, False)],\n",
      "    1: [   (0.3333333333333333, 13, 0.0, False),\n",
      "           (0.3333333333333333, 14, 0.0, False),\n",
      "           (0.3333333333333333, 15, 1.0, True)],\n",
      "    2: [   (0.3333333333333333, 14, 0.0, False),\n",
      "           (0.3333333333333333, 15, 1.0, True),\n",
      "           (0.3333333333333333, 10, 0.0, False)],\n",
      "    3: [   (0.3333333333333333, 15, 1.0, True),\n",
      "           (0.3333333333333333, 10, 0.0, False),\n",
      "           (0.3333333333333333, 13, 0.0, False)]}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(env.P[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90388f6d-1f8b-4691-adc1-16576fe70518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "newstate 4\n",
      "reward 0.0\n",
      "done False\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "new_state, r, done, info = env.step(1)\n",
    "# step takes the indexed action\n",
    "\n",
    "print('newstate {}'.format(new_state))\n",
    "print('reward {}'.format(r))\n",
    "print('done {}'.format(done))\n",
    "print()\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e54304-e053-4aec-859b-69c6419501aa",
   "metadata": {},
   "source": [
    "This is basic code to take a step without learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20c35fcb-06a8-4148-9183-45ae9dd7e940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 3 timesteps\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for i in range(40):\n",
    "    # env.render()\n",
    "    new_state, r, done, info = env.step(env.action_space.sample())\n",
    "    # pp.pprint(env.P[new_state])\n",
    "    if done: # done is used for both failed case and success case\n",
    "        print(\"Episode finished after {} timesteps\".format(i+1))\n",
    "        env.render() \n",
    "        break\n",
    "    # print('Observation : ' + str(new_state))\n",
    "    # print('Reward      : ' + str(r))\n",
    "    # print('Done        : ' + str(done))\n",
    "    # print('---------------------')\n",
    "    # print(env.s) # this is the current state number\n",
    "    # print(env.isd[env.s]) # isd is initial state distribution\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82bc9bef-0e8a-4827-a656-b3a37a0de269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P is a dictionary of dictionary of lists\n",
    "# P[s][a] == [(prob, next_state, reward, terminal), ...]\n",
    "# I isd is a list or array of length nS\n",
    "# isd == [0., 0., 1., 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c94aa-f171-40e5-8087-681b1fdc7d4c",
   "metadata": {},
   "source": [
    "https://gist.github.com/artonge/0cebe66a00d2204c1fb1d0694c6ee014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f31a8c64-4598-414e-aed8-9b3365238070",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "8\n",
      "12\n",
      "[{'state': 0, 'action': 3, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 4, 'action': 0, 'reward': 0.0}, {'state': 8, 'action': 0, 'reward': 0.0}]\n",
      "[{'state': 0, 'action': 3, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 4, 'action': 0, 'reward': 0.0}, {'state': 8, 'action': 0, 'reward': 0.0}]\n",
      "[{'state': 0, 'action': 3, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 4, 'action': 0, 'reward': 0.0}, {'state': 8, 'action': 0, 'reward': 0.0}]\n",
      "[{'state': 0, 'action': 3, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 4, 'action': 0, 'reward': 0.0}, {'state': 8, 'action': 0, 'reward': 0.0}]\n",
      "[{'state': 0, 'action': 3, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 4, 'action': 0, 'reward': 0.0}, {'state': 8, 'action': 0, 'reward': 0.0}]\n",
      "[{'state': 0, 'action': 3, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 0, 'action': 0, 'reward': 0.0}, {'state': 4, 'action': 0, 'reward': 0.0}, {'state': 8, 'action': 0, 'reward': 0.0}]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGdCAYAAAD0e7I1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuQElEQVR4nO3de3wU9b3/8ffktlxMVkNINikhpIoiBimXAsYiUCAQFUFoBaEKHg5Hj4hcD0p7lKCn3KxIFUUPpSDesCqhWjlCEAG5qQRQblXAAKEkjWDMJhA3CczvD39sXZJAluxmd4fX8/GYx6Pzne9MPvN9WPftd78za5imaQoAAMCiwgJdAAAAgD8RdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKVFBLqAYHD27FkdP35c0dHRMgwj0OUAAIA6ME1TpaWlSkpKUlhY7fM3hB1Jx48fV3JycqDLAAAAlyA/P18tWrSo9ThhR1J0dLSkHwYrJiYmwNUAAIC6cDqdSk5Odn+O14awI7m/uoqJiSHsAAAQYi62BIUFygAAwNICGnZmzZqln//854qOjlZ8fLwGDRqkL7/80qOPaZrKyspSUlKSGjdurJ49e2rv3r0efVwul8aNG6e4uDg1bdpUd9xxh44dO9aQtwIAAIJUQMPOhg0bNHbsWG3btk05OTmqqqpSRkaGTp065e4zd+5czZs3TwsWLNBnn30mh8Ohvn37qrS01N1nwoQJys7O1vLly7Vp0yaVlZXp9ttv15kzZwJxWwAAIIgYpmmagS7inG+++Ubx8fHasGGDbrnlFpmmqaSkJE2YMEGPPPKIpB9mcRISEjRnzhzdf//9KikpUfPmzfXKK69o6NChkv71dNWqVavUr1+/i/5dp9Mpu92ukpIS1uwAABAi6vr5HVRrdkpKSiRJsbGxkqS8vDwVFhYqIyPD3cdms6lHjx7asmWLJCk3N1eVlZUefZKSkpSWlubucz6XyyWn0+mxAQAAawqasGOapiZNmqRf/OIXSktLkyQVFhZKkhISEjz6JiQkuI8VFhYqKipKV111Va19zjdr1izZ7Xb3xjt2AACwrqAJOw899JC++OILvfHGG9WOnf9ImWmaF33M7EJ9pk2bppKSEveWn59/6YUDAICgFhRhZ9y4cXr33Xf10UcfebwB0eFwSFK1GZqioiL3bI/D4VBFRYWKi4tr7XM+m83mfqcO79YBAMDaAhp2TNPUQw89pBUrVmjdunVKTU31OJ6amiqHw6GcnBx3W0VFhTZs2KD09HRJUqdOnRQZGenRp6CgQHv27HH3AQAAl6+AvkF57Nixev311/XXv/5V0dHR7hkcu92uxo0byzAMTZgwQTNnzlTr1q3VunVrzZw5U02aNNHw4cPdfUePHq3JkyerWbNmio2N1ZQpU9SuXTv16dMnkLcHAACCQEDDzsKFCyVJPXv29GhfsmSJRo0aJUmaOnWqysvL9eCDD6q4uFhdu3bVmjVrPH4H45lnnlFERITuuusulZeXq3fv3lq6dKnCw8Mb6lYAAECQCqr37AQK79kBACD0hOR7dgAAAHyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACwtoGFn48aNGjBggJKSkmQYhlauXOlx3DCMGrennnrK3adnz57Vjg8bNqyB7wQAAASrgIadU6dOqX379lqwYEGNxwsKCjy2P//5zzIMQ0OGDPHoN2bMGI9+L730UkOUDwAAQkBEIP94ZmamMjMzaz3ucDg89v/617+qV69e+ulPf+rR3qRJk2p9AQAApBBas/PPf/5T77//vkaPHl3t2Guvvaa4uDjdcMMNmjJlikpLSy94LZfLJafT6bEBAABrCujMjjdefvllRUdHa/DgwR7tI0aMUGpqqhwOh/bs2aNp06bp888/V05OTq3XmjVrlmbMmOHvkgEAQBAwTNM0A12E9MNi5OzsbA0aNKjG423atFHfvn313HPPXfA6ubm56ty5s3Jzc9WxY8ca+7hcLrlcLve+0+lUcnKySkpKFBMTc8n3AAAAGo7T6ZTdbr/o53dIzOx8/PHH+vLLL/Xmm29etG/Hjh0VGRmpAwcO1Bp2bDabbDabr8sEAABBKCTW7CxevFidOnVS+/btL9p37969qqysVGJiYgNUBgAAgl1AZ3bKysp08OBB935eXp527dql2NhYtWzZUtIPU1RvvfWWnn766WrnHzp0SK+99ppuvfVWxcXFad++fZo8ebI6dOigm2++ucHuAwAABK+Ahp3t27erV69e7v1JkyZJkkaOHKmlS5dKkpYvXy7TNHX33XdXOz8qKkoffvih/vjHP6qsrEzJycm67bbbNH36dIWHhzfIPQAAgOAWNAuUA6muC5wAAEDwqOvnd0is2QEAALhUhB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBpAQ07Gzdu1IABA5SUlCTDMLRy5UqP46NGjZJhGB5bt27dPPq4XC6NGzdOcXFxatq0qe644w4dO3asAe8CAAAEs4CGnVOnTql9+/ZasGBBrX369++vgoIC97Zq1SqP4xMmTFB2draWL1+uTZs2qaysTLfffrvOnDnj7/IBAEAIiAjkH8/MzFRmZuYF+9hsNjkcjhqPlZSUaPHixXrllVfUp08fSdKrr76q5ORkrV27Vv369fN5zQAAILQE/Zqd9evXKz4+Xtdee63GjBmjoqIi97Hc3FxVVlYqIyPD3ZaUlKS0tDRt2bKl1mu6XC45nU6PDQAAWFNQh53MzEy99tprWrdunZ5++ml99tln+uUvfymXyyVJKiwsVFRUlK666iqP8xISElRYWFjrdWfNmiW73e7ekpOT/XofAAAgcAL6NdbFDB061P2/09LS1LlzZ6WkpOj999/X4MGDaz3PNE0ZhlHr8WnTpmnSpEnufafTSeABAMCignpm53yJiYlKSUnRgQMHJEkOh0MVFRUqLi726FdUVKSEhIRar2Oz2RQTE+OxAQAAawqpsHPy5Enl5+crMTFRktSpUydFRkYqJyfH3aegoEB79uxRenp6oMoEAABBJKBfY5WVlengwYPu/by8PO3atUuxsbGKjY1VVlaWhgwZosTERB0+fFi//e1vFRcXpzvvvFOSZLfbNXr0aE2ePFnNmjVTbGyspkyZonbt2rmfzgIAAJe3gIad7du3q1evXu79c+toRo4cqYULF2r37t1atmyZvvvuOyUmJqpXr1568803FR0d7T7nmWeeUUREhO666y6Vl5erd+/eWrp0qcLDwxv8fgAAQPAxTNM0A11EoDmdTtntdpWUlLB+BwCAEFHXz++QWrMDAADgLcIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNK/DTnl5uU6fPu3eP3LkiObPn681a9b4tDAAAABf8DrsDBw4UMuWLZMkfffdd+ratauefvppDRw4UAsXLvR5gQAAAPXhddjZsWOHunfvLkl6++23lZCQoCNHjmjZsmV69tlnfV4gAABAfXgddk6fPq3o6GhJ0po1azR48GCFhYWpW7duOnLkiM8LBAAAqA+vw84111yjlStXKj8/X6tXr1ZGRoYkqaioSDExMT4vEAAAoD68DjuPP/64pkyZolatWqlLly666aabJP0wy9OhQwevrrVx40YNGDBASUlJMgxDK1eudB+rrKzUI488onbt2qlp06ZKSkrSvffeq+PHj3tco2fPnjIMw2MbNmyYt7cFAAAsyuuw86tf/UpHjx7V9u3btXr1and779699cwzz3h1rVOnTql9+/ZasGBBtWOnT5/Wjh079Nhjj2nHjh1asWKFvvrqK91xxx3V+o4ZM0YFBQXu7aWXXvL2tgAAgEVFXMpJDodDDodD+fn5MgxDLVq0UJcuXby+TmZmpjIzM2s8ZrfblZOT49H23HPPqUuXLjp69Khatmzpbm/SpIkcDofXfx8AAFif1zM7VVVVeuyxx2S329WqVSulpKTIbrfrv//7v1VZWemPGt1KSkpkGIauvPJKj/bXXntNcXFxuuGGGzRlyhSVlpZe8Doul0tOp9NjAwAA1uT1zM5DDz2k7OxszZ07171eZ+vWrcrKytKJEyf04osv+rxISfr+++/16KOPavjw4R4LoUeMGKHU1FQ5HA7t2bNH06ZN0+eff15tVujHZs2apRkzZvilTgAAEFwM0zRNb06w2+1avnx5ta+f/u///k/Dhg1TSUnJpRViGMrOztagQYOqHausrNSvf/1rHT16VOvXr7/gU1+5ubnq3LmzcnNz1bFjxxr7uFwuuVwu977T6VRycrJKSkp4ogwAgBDhdDplt9sv+vnt9cxOo0aN1KpVq2rtrVq1UlRUlLeXu6jKykrdddddysvL07p16y4aRjp27KjIyEgdOHCg1rBjs9lks9l8XisAAAg+Xq/ZGTt2rJ588kmPmRGXy6Xf//73euihh3xa3Lmgc+DAAa1du1bNmjW76Dl79+5VZWWlEhMTfVoLAAAITV7P7OzcuVMffvihWrRoofbt20uSPv/8c1VUVKh3794aPHiwu++KFSsueK2ysjIdPHjQvZ+Xl6ddu3YpNjZWSUlJ+tWvfqUdO3bob3/7m86cOaPCwkJJUmxsrKKionTo0CG99tpruvXWWxUXF6d9+/Zp8uTJ6tChg26++WZvbw0AAFiQ12t27rvvvjr3XbJkyQWPr1+/Xr169arWPnLkSGVlZSk1NbXG8z766CP17NlT+fn5+s1vfqM9e/aorKxMycnJuu222zR9+nTFxsbWuc66fucHAACCR10/v70OO1ZE2AEAIPTU9fPb6zU70g/v2lm7dq1eeukl9zttjh8/rrKyskurFgAAwE+8XrNz5MgR9e/fX0ePHpXL5VLfvn0VHR2tuXPn6vvvv/fbe3YAAAAuhdczO+PHj1fnzp1VXFysxo0bu9vvvPNOffjhhz4tDgAAoL68ntnZtGmTNm/eXO2dOikpKfrHP/7hs8IAAAB8weuZnbNnz+rMmTPV2o8dO6bo6GifFAUAAOArXoedvn37av78+e59wzBUVlam6dOn69Zbb/VlbQAAAPXm9aPnx48fV69evRQeHq4DBw6oc+fOOnDggOLi4rRx40bFx8f7q1a/4dFzAABCj99+GyspKUm7du3S8uXLlZubq7Nnz2r06NEaMWKEx4JlAACAYOD1zM7GjRuVnp6uiAjPnFRVVaUtW7bolltu8WmBDYGZHQAAQo/fXirYq1cvffvtt9XaS0pKavzpBwAAgEDyOuyYpinDMKq1nzx5Uk2bNvVJUQAAAL5S5zU7537N3DAMjRo1SjabzX3szJkz+uKLL5Senu77CgEAAOqhzmHHbrdL+mFmJzo62mMxclRUlLp166YxY8b4vkIAAIB6qHPYWbJkiSSpVatWmjJlCl9ZAQCAkOD1mp2pU6d6rNk5cuSI5s+frzVr1vi0MAAAAF/wOuwMHDhQy5YtkyR999136tKli55++mkNHDhQCxcu9HmBAAAA9eF12NmxY4e6d+8uSXr77bflcDh05MgRLVu2TM8++6zPCwQAAKgPr8PO6dOn3T/4uWbNGg0ePFhhYWHq1q2bjhw54vMCAQAA6sPrsHPNNddo5cqVys/P1+rVq5WRkSFJKioq4u3DAAAg6Hgddh5//HFNmTJFrVq1UteuXXXTTTdJ+mGWp0OHDj4vEAAAoD68/m0sSSosLFRBQYHat2+vsLAf8tKnn36qmJgYtWnTxudF+hu/jQUAQOjx26+eS5LD4ZDD4fBo69Kly6VcCgAAwK+8/hoLAAAglBB2AACApRF2AACApdUp7HTs2FHFxcWSpCeeeEKnT5/2a1EAAAC+Uqews3//fp06dUqSNGPGDJWVlfm1KAAAAF+p09NYP/vZz3TffffpF7/4hUzT1B/+8AddccUVNfZ9/PHHfVogAABAfdTpPTtffvmlpk+frkOHDmnHjh1q27atIiKq5yTDMLRjxw6/FOpPvGcHAIDQU9fPb69fKhgWFqbCwkLFx8fXu8hgQdgBACD0+O2lgmfPnq1XYQAAAA3pkt6gfOjQIc2fP1/79++XYRi6/vrrNX78eF199dW+rg8AAKBevH7PzurVq9W2bVt9+umnuvHGG5WWlqZPPvlEN9xwg3Jycry61saNGzVgwAAlJSXJMAytXLnS47hpmsrKylJSUpIaN26snj17au/evR59XC6Xxo0bp7i4ODVt2lR33HGHjh075u1tAQAAi/I67Dz66KOaOHGiPvnkE82bN0/PPPOMPvnkE02YMEGPPPKIV9c6deqU2rdvrwULFtR4fO7cuZo3b54WLFigzz77TA6HQ3379lVpaam7z4QJE5Sdna3ly5dr06ZNKisr0+23364zZ854e2sAAMCCvF6g3KhRI+3evVutW7f2aP/qq69044036vvvv7+0QgxD2dnZGjRokKQfZnWSkpI8QpTL5VJCQoLmzJmj+++/XyUlJWrevLleeeUVDR06VJJ0/PhxJScna9WqVerXr1+d/jYLlAEACD11/fz2emanefPm2rVrV7X2Xbt2+fQJrby8PBUWFiojI8PdZrPZ1KNHD23ZskWSlJubq8rKSo8+SUlJSktLc/epicvlktPp9NgAAIA1eb1AecyYMfqP//gPff3110pPT5dhGNq0aZPmzJmjyZMn+6ywwsJCSVJCQoJHe0JCgo4cOeLuExUVpauuuqpan3Pn12TWrFmaMWOGz2oFAADBy+uw89hjjyk6OlpPP/20pk2bJumH2ZSsrCw9/PDDPi/QMAyPfdM0q7Wd72J9pk2bpkmTJrn3nU6nkpOT61coAAAISl6HHcMwNHHiRE2cONG9UDg6OtrnhTkcDkk/zN4kJia624uKityzPQ6HQxUVFSouLvaY3SkqKlJ6enqt17bZbLLZbD6vGQAABB+v1+z8WHR0tF+CjiSlpqbK4XB4PM5eUVGhDRs2uINMp06dFBkZ6dGnoKBAe/bsuWDYAQAAl49Leqmgr5SVlengwYPu/by8PO3atUuxsbFq2bKlJkyYoJkzZ6p169Zq3bq1Zs6cqSZNmmj48OGSJLvdrtGjR2vy5Mlq1qyZYmNjNWXKFLVr1059+vQJ1G0BAIAgEtCws337dvXq1cu9f24dzciRI7V06VJNnTpV5eXlevDBB1VcXKyuXbtqzZo1HrNJzzzzjCIiInTXXXepvLxcvXv31tKlSxUeHt7g9wMAAIKP1+/ZsSLeswMAQOjxy3t2Kisr1atXL3311Vf1LhAAAKAheBV2IiMjtWfPnos++g0AABAsvH4a695779XixYv9UQsAAIDPeb1AuaKiQn/605+Uk5Ojzp07q2nTph7H582b57PiAAAA6svrsLNnzx517NhRkqqt3eHrLQAAEGy8DjsfffSRP+oAAADwi0t+g/LBgwe1evVqlZeXS/rh96gAAACCjddh5+TJk+rdu7euvfZa3XrrrSooKJAk/fu//7tPf/UcAADAF7wOOxMnTlRkZKSOHj2qJk2auNuHDh2qDz74wKfFAQAA1JfXa3bWrFmj1atXq0WLFh7trVu31pEjR3xWGAAAgC94PbNz6tQpjxmdc06cOCGbzeaTogAAAHzF67Bzyy23aNmyZe59wzB09uxZPfXUUx4/6gkAABAMvP4a66mnnlLPnj21fft2VVRUaOrUqdq7d6++/fZbbd682R81AgAAXDKvZ3batm2rL774Ql26dFHfvn116tQpDR48WDt37tTVV1/tjxoBAAAumWHygpw6/0Q8AAAIHnX9/Pb6ayxJKi4u1uLFi7V//34ZhqHrr79e9913n2JjYy+5YAAAAH/w+musDRs2KDU1Vc8++6yKi4v17bff6tlnn1Vqaqo2bNjgjxoBAAAumddfY6WlpSk9PV0LFy5UeHi4JOnMmTN68MEHtXnzZu3Zs8cvhfoTX2MBABB66vr57fXMzqFDhzR58mR30JGk8PBwTZo0SYcOHbq0agEAAPzE67DTsWNH7d+/v1r7/v379bOf/cwXNQEAAPhMnRYof/HFF+7//fDDD2v8+PE6ePCgunXrJknatm2bnn/+ec2ePds/VQIAAFyiOq3ZCQsLk2EYulhXwzB05swZnxXXUFizAwBA6PHpo+d5eXk+KwwAAKAh1SnspKSk+LsOAAAAv7iklwr+4x//0ObNm1VUVKSzZ896HHv44Yd9UhgAAIAveB12lixZogceeEBRUVFq1qyZDMNwHzMMg7ADAACCitcvFUxOTtYDDzygadOmKSzM6yfXgxILlAEACD1+e6ng6dOnNWzYMMsEHQAAYG1eJ5bRo0frrbfe8kctAAAAPuf111hnzpzR7bffrvLycrVr106RkZEex+fNm+fTAhsCX2MBABB6fPqenR+bOXOmVq9ereuuu06Sqi1QBgAACCZeh5158+bpz3/+s0aNGuWHcgAAAHzL6zU7NptNN998sz9qqVGrVq1kGEa1bezYsZKkUaNGVTt27je7AAAAvA4748eP13PPPeePWmr02WefqaCgwL3l5ORIkn7961+7+/Tv39+jz6pVqxqsPgAAENy8/hrr008/1bp16/S3v/1NN9xwQ7UFyitWrPBZcZLUvHlzj/3Zs2fr6quvVo8ePdxtNptNDofDp38XAABYg9dh58orr9TgwYP9UctFVVRU6NVXX9WkSZM8FkOvX79e8fHxuvLKK9WjRw/9/ve/V3x8fK3Xcblccrlc7n2n0+nXugEAQOB4/eh5IP3lL3/R8OHDdfToUSUlJUmS3nzzTV1xxRVKSUlRXl6eHnvsMVVVVSk3N1c2m63G62RlZWnGjBnV2nn0HACA0FHXR89DKuz069dPUVFReu+992rtU1BQoJSUFC1fvrzWGaiaZnaSk5MJOwAAhBC/vWcnNTX1gu/T+frrr729ZJ0cOXJEa9euveiaoMTERKWkpOjAgQO19rHZbLXO+gAAAGvxOuxMmDDBY7+yslI7d+7UBx98oP/6r//yVV3VLFmyRPHx8brtttsu2O/kyZPKz89XYmKi32oBAAChw+uwM378+Brbn3/+eW3fvr3eBdXk7NmzWrJkiUaOHKmIiH+VXFZWpqysLA0ZMkSJiYk6fPiwfvvb3youLk533nmnX2oBAAChxWc/XZ6Zmal33nnHV5fzsHbtWh09elT/9m//5tEeHh6u3bt3a+DAgbr22ms1cuRIXXvttdq6dauio6P9UgsAAAgtXs/s1Obtt99WbGysry7nISMjQzWto27cuLFWr17tl78JAACsweuw06FDB48FyqZpqrCwUN98841eeOEFnxYHAABQX16HnUGDBnnsh4WFqXnz5urZs6fatGnjq7oAAAB8IqTes+MvdX1OHwAABI+6fn77bIEyAABAMKrz11hhYWEXfJmgJBmGoaqqqnoXBQAA4Ct1DjvZ2dm1HtuyZYuee+65Gp+YAgAACKQ6h52BAwdWa/v73/+uadOm6b333tOIESP05JNP+rQ4AACA+rqkNTvHjx/XmDFjdOONN6qqqkq7du3Syy+/rJYtW/q6PgAAgHrxKuyUlJTokUce0TXXXKO9e/fqww8/1Hvvvae0tDR/1QcAAFAvdf4aa+7cuZozZ44cDofeeOONGr/WAgAACDZ1fs9OWFiYGjdurD59+ig8PLzWfitWrPBZcQ2F9+wAABB66vr5XeeZnXvvvfeij54DAAAEmzqHnaVLl/qxDAAAAP/gDcoAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSgjrsZGVlyTAMj83hcLiPm6aprKwsJSUlqXHjxurZs6f27t0bwIoBAECwCeqwI0k33HCDCgoK3Nvu3bvdx+bOnat58+ZpwYIF+uyzz+RwONS3b1+VlpYGsGIAABBMgj7sREREyOFwuLfmzZtL+mFWZ/78+frd736nwYMHKy0tTS+//LJOnz6t119/PcBVAwCAYBH0YefAgQNKSkpSamqqhg0bpq+//lqSlJeXp8LCQmVkZLj72mw29ejRQ1u2bLngNV0ul5xOp8cGAACsKajDTteuXbVs2TKtXr1aixYtUmFhodLT03Xy5EkVFhZKkhISEjzOSUhIcB+rzaxZs2S3291bcnKy3+4BAAAEVlCHnczMTA0ZMkTt2rVTnz599P7770uSXn75ZXcfwzA8zjFNs1rb+aZNm6aSkhL3lp+f7/viAQBAUAjqsHO+pk2bql27djpw4ID7qazzZ3GKioqqzfacz2azKSYmxmMDAADWFFJhx+Vyaf/+/UpMTFRqaqocDodycnLcxysqKrRhwwalp6cHsEoAABBMIgJdwIVMmTJFAwYMUMuWLVVUVKT/+Z//kdPp1MiRI2UYhiZMmKCZM2eqdevWat26tWbOnKkmTZpo+PDhgS4dAAAEiaAOO8eOHdPdd9+tEydOqHnz5urWrZu2bdumlJQUSdLUqVNVXl6uBx98UMXFxeratavWrFmj6OjoAFcOAACChWGaphnoIgLN6XTKbrerpKSE9TsAAISIun5+h9SaHQAAAG8RdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKUFddiZNWuWfv7znys6Olrx8fEaNGiQvvzyS48+o0aNkmEYHlu3bt0CVDEAAAg2QR12NmzYoLFjx2rbtm3KyclRVVWVMjIydOrUKY9+/fv3V0FBgXtbtWpVgCoGAADBJiLQBVzIBx984LG/ZMkSxcfHKzc3V7fccou73WazyeFwNHR5AAAgBAT1zM75SkpKJEmxsbEe7evXr1d8fLyuvfZajRkzRkVFRRe8jsvlktPp9NgAAIA1GaZpmoEuoi5M09TAgQNVXFysjz/+2N3+5ptv6oorrlBKSory8vL02GOPqaqqSrm5ubLZbDVeKysrSzNmzKjWXlJSopiYGL/dAwAA8B2n0ym73X7Rz++QCTtjx47V+++/r02bNqlFixa19isoKFBKSoqWL1+uwYMH19jH5XLJ5XK5951Op5KTkwk7AACEkLqGnaBes3POuHHj9O6772rjxo0XDDqSlJiYqJSUFB04cKDWPjabrdZZHwAAYC1BHXZM09S4ceOUnZ2t9evXKzU19aLnnDx5Uvn5+UpMTGyACgEAQLAL6gXKY8eO1auvvqrXX39d0dHRKiwsVGFhocrLyyVJZWVlmjJlirZu3arDhw9r/fr1GjBggOLi4nTnnXcGuHoAABAMgnrNjmEYNbYvWbJEo0aNUnl5uQYNGqSdO3fqu+++U2Jionr16qUnn3xSycnJdf47df3ODwAABA9LrNm5WA5r3LixVq9e3UDVAACAUBTUX2MBAADUF2EHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYmmXCzgsvvKDU1FQ1atRInTp10scffxzokgAAQBCwRNh58803NWHCBP3ud7/Tzp071b17d2VmZuro0aOBLg0AAASYYZqmGegi6qtr167q2LGjFi5c6G67/vrrNWjQIM2aNeui5zudTtntdpWUlCgmJsafpQIAAB+p6+d3yM/sVFRUKDc3VxkZGR7tGRkZ2rJlS43nuFwuOZ1Ojw0AAFhTyIedEydO6MyZM0pISPBoT0hIUGFhYY3nzJo1S3a73b0lJyc3RKkAACAAQj7snGMYhse+aZrV2s6ZNm2aSkpK3Ft+fn5DlAgAAAIgItAF1FdcXJzCw8OrzeIUFRVVm+05x2azyWazNUR5AAAgwEJ+ZicqKkqdOnVSTk6OR3tOTo7S09MDVBUAAAgWIT+zI0mTJk3SPffco86dO+umm27S//7v/+ro0aN64IEHAl0aAAAIMEuEnaFDh+rkyZN64oknVFBQoLS0NK1atUopKSmBLg0AAASYJd6zU1+8ZwcAgNBz2bxnBwAA4EIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIs8XMR9XXuJdJOpzPAlQAAgLo697l9sR+DIOxIKi0tlSQlJycHuBIAAOCt0tJS2e32Wo/z21iSzp49q+PHjys6OlqGYQS6nIBzOp1KTk5Wfn4+vxXmR4xzw2CcGwbj3DAYZ0+maaq0tFRJSUkKC6t9ZQ4zO5LCwsLUokWLQJcRdGJiYvg/UwNgnBsG49wwGOeGwTj/y4VmdM5hgTIAALA0wg4AALA0wg6qsdlsmj59umw2W6BLsTTGuWEwzg2DcW4YjPOlYYEyAACwNGZ2AACApRF2AACApRF2AACApRF2AACApRF2LkPFxcW65557ZLfbZbfbdc899+i777674DmmaSorK0tJSUlq3Lixevbsqb1799baNzMzU4ZhaOXKlb6/gRDhj3H+9ttvNW7cOF133XVq0qSJWrZsqYcfflglJSV+vpvg8cILLyg1NVWNGjVSp06d9PHHH1+w/4YNG9SpUyc1atRIP/3pT/Xiiy9W6/POO++obdu2stlsatu2rbKzs/1Vfsjw9TgvWrRI3bt311VXXaWrrrpKffr00aeffurPWwgZ/vhn+pzly5fLMAwNGjTIx1WHGBOXnf79+5tpaWnmli1bzC1btphpaWnm7bfffsFzZs+ebUZHR5vvvPOOuXv3bnPo0KFmYmKi6XQ6q/WdN2+emZmZaUoys7Oz/XQXwc8f47x7925z8ODB5rvvvmsePHjQ/PDDD83WrVubQ4YMaYhbCrjly5ebkZGR5qJFi8x9+/aZ48ePN5s2bWoeOXKkxv5ff/212aRJE3P8+PHmvn37zEWLFpmRkZHm22+/7e6zZcsWMzw83Jw5c6a5f/9+c+bMmWZERIS5bdu2hrqtoOOPcR4+fLj5/PPPmzt37jT3799v3nfffabdbjePHTvWULcVlPwx1uccPnzY/MlPfmJ2797dHDhwoJ/vJLgRdi4z+/btMyV5/It869atpiTz73//e43nnD171nQ4HObs2bPdbd9//71pt9vNF1980aPvrl27zBYtWpgFBQWXddjx9zj/2F/+8hczKirKrKys9N0NBKkuXbqYDzzwgEdbmzZtzEcffbTG/lOnTjXbtGnj0Xb//feb3bp1c+/fddddZv/+/T369OvXzxw2bJiPqg49/hjn81VVVZnR0dHmyy+/XP+CQ5i/xrqqqsq8+eabzT/96U/myJEjL/uww9dYl5mtW7fKbrera9eu7rZu3brJbrdry5YtNZ6Tl5enwsJCZWRkuNtsNpt69Ojhcc7p06d19913a8GCBXI4HP67iRDgz3E+X0lJiWJiYhQRYe2fuquoqFBubq7H+EhSRkZGreOzdevWav379eun7du3q7Ky8oJ9LjTmVuavcT7f6dOnVVlZqdjYWN8UHoL8OdZPPPGEmjdvrtGjR/u+8BBE2LnMFBYWKj4+vlp7fHy8CgsLaz1HkhISEjzaExISPM6ZOHGi0tPTNXDgQB9WHJr8Oc4/dvLkST355JO6//7761lx8Dtx4oTOnDnj1fgUFhbW2L+qqkonTpy4YJ/arml1/hrn8z366KP6yU9+oj59+vim8BDkr7HevHmzFi9erEWLFvmn8BBE2LGIrKwsGYZxwW379u2SJMMwqp1vmmaN7T92/vEfn/Puu+9q3bp1mj9/vm9uKEgFepx/zOl06rbbblPbtm01ffr0etxVaKnr+Fyo//nt3l7zcuCPcT5n7ty5euONN7RixQo1atTIB9WGNl+OdWlpqX7zm99o0aJFiouL832xIcra896XkYceekjDhg27YJ9WrVrpiy++0D//+c9qx7755ptq/7VwzrmvpAoLC5WYmOhuLyoqcp+zbt06HTp0SFdeeaXHuUOGDFH37t21fv16L+4meAV6nM8pLS1V//79dcUVVyg7O1uRkZHe3krIiYuLU3h4eLX/4q1pfM5xOBw19o+IiFCzZs0u2Ke2a1qdv8b5nD/84Q+aOXOm1q5dqxtvvNG3xYcYf4z13r17dfjwYQ0YMMB9/OzZs5KkiIgIffnll7r66qt9fCchIEBrhRAg5xbOfvLJJ+62bdu21Wnh7Jw5c9xtLpfLY+FsQUGBuXv3bo9NkvnHP/7R/Prrr/17U0HIX+NsmqZZUlJiduvWzezRo4d56tQp/91EEOrSpYv5n//5nx5t119//QUXc15//fUebQ888EC1BcqZmZkeffr373/ZL1D29TibpmnOnTvXjImJMbdu3erbgkOYr8e6vLy82r+LBw4caP7yl780d+/ebbpcLv/cSJAj7FyG+vfvb954443m1q1bza1bt5rt2rWr9kj0ddddZ65YscK9P3v2bNNut5srVqwwd+/ebd599921Pnp+ji7jp7FM0z/j7HQ6za5du5rt2rUzDx48aBYUFLi3qqqqBr2/QDj3mO7ixYvNffv2mRMmTDCbNm1qHj582DRN03z00UfNe+65x93/3GO6EydONPft22cuXry42mO6mzdvNsPDw83Zs2eb+/fvN2fPns2j534Y5zlz5phRUVHm22+/7fHPbWlpaYPfXzDxx1ifj6exCDuXpZMnT5ojRowwo6OjzejoaHPEiBFmcXGxRx9J5pIlS9z7Z8+eNadPn246HA7TZrOZt9xyi7l79+4L/p3LPez4Y5w/+ugjU1KNW15eXsPcWIA9//zzZkpKihkVFWV27NjR3LBhg/vYyJEjzR49enj0X79+vdmhQwczKirKbNWqlblw4cJq13zrrbfM6667zoyMjDTbtGljvvPOO/6+jaDn63FOSUmp8Z/b6dOnN8DdBDd//DP9Y4Qd0zRM8/+vbAIAALAgnsYCAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACW9v8A6byJzWYjpqMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from random import randint\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def learn(episodeCount):\n",
    "    for i_episode in range(episodeCount):\n",
    "        obs = env.reset()\n",
    "        start = True\n",
    "\n",
    "# Compute the decay of the exploration\n",
    "\n",
    "# decayX = 0.02\n",
    "# decayY = 20\n",
    "# why are we doing max? is this the max value when taking an action from values?\n",
    "# decay = max(-i_episode*decayX+decayY, 10/(i_episode+1))\n",
    "# this is the monte carlo\n",
    "        doEpisodeMC(obs, i_episode, start)\n",
    "\n",
    "\n",
    "\n",
    "def doEpisodeMC(obs, i_episode, start):\n",
    "    episodeStatesActions = []\n",
    "    states_in_episode = []\n",
    "    totalRewards = 0\n",
    "    while(True):\n",
    "        if start:\n",
    "            action = np.random.choice([0,3])\n",
    "            start=False\n",
    "        else:\n",
    "            action = policy[obs]\n",
    "        next_state, reward, end, _ = env.step(action)\n",
    "        print(next_state)\n",
    "# state = getState(obs)  # Get the state\n",
    "# action = policy(state, decay)  # Get the action\n",
    "        episodeStatesActions.append({'state': obs, 'action': action, 'reward': reward})  # Save state and action to episodeStatesActions\n",
    "# obs, reward, done, _ = env.step(action)  # Apply the action\n",
    "        states_in_episode.append(obs)\n",
    "        totalRewards += reward  # Update total reward for this episode\n",
    "        if end:\n",
    "            break\n",
    "        obs = next_state\n",
    "\n",
    "    for dicti in episodeStatesActions:\n",
    "        first_occurence_idx = next(i for i,d in enumerate(episodeStatesActions) if d==dicti)\n",
    "        print(episodeStatesActions)\n",
    "        # new_val = (current_val * N + reward) / N + 1\n",
    "        g = 0\n",
    "        for event in episodeStatesActions:\n",
    "            g += event['reward']\n",
    "        returns_sum[d] += g\n",
    "        returns_count[(st,act)] += 1.0\n",
    "        Q[st][act] = returns_sum[(st,act)] / returns_count[(st,act)]\n",
    "\n",
    "# @param state <string> the state to update\n",
    "# @param action <int> the action to update\n",
    "# @param G <int> the reward\n",
    "# the third argument is the total rewards minus the enumeration, the more number of steps you've taken the lesser rewards is passed in\n",
    "def updatePolicyMC(state, action, G):\n",
    "    a = history[state][action]\n",
    "# print(a)\n",
    "# this is where you update the value or the policy\n",
    "# new_val = (current_val * N + reward) / N + 1\n",
    "# where N is the number of times the stae was visited\n",
    "# meaning this position has quite a significant weightage in reaching the goal\n",
    "# we find the average of the rewards\n",
    "    a['value'] = (a['value'] * a['count'] + G) / (a['count'] + 1)\n",
    "    a['count'] += 1\n",
    "\n",
    "\n",
    "# @param obs <[float]> the observation to convert into a state\n",
    "# @return the state associated to the observation_space\n",
    "# If the set of observations where never met, create the state\n",
    "# The function reduces the number of possible states\n",
    "def getState(obs):\n",
    "    state = ''\n",
    "    state += str(math.floor(obs))\n",
    "    return state\n",
    "\n",
    "\n",
    "# @param state <string>\n",
    "# @param decay <int>\n",
    "# @return an action\n",
    "# The policy progressivly stops exploration and gets greedy\n",
    "# def policy(state, decay):\n",
    "# \t# Get the less explored action and the most valued action\n",
    "#     # you always first randomly sample the action space, the max valued action and the least explored action\n",
    "# \tmaxValueAction = env.action_space.sample()\n",
    "# \tminCountAction = env.action_space.sample()\n",
    "# \tif not state in history:  # If state does not existe, create it\n",
    "# \t\thistory[state] = []\n",
    "# \t\tfor _ in range(env.action_space.n):\n",
    "#             history[state].append({'count':0, 'value':0})\n",
    "#             # so the state has a list\n",
    "#             # and for each action available in general, the state stores the number of times action taken and the value associated with it\n",
    "# \t\t\t# exploring states we make sure to start the state values from 0\n",
    "\n",
    "# \tstateValues = history[state] # get the action values for that state\n",
    "# \tfor action in range(env.action_space.n):\n",
    "# \t\tif stateValues[maxValueAction]['value'] < stateValues[action]['value']: # see if the random action has a better value or the learnt one\n",
    "# \t\t\tmaxValueAction = action # take the higher valued action\n",
    "# \t\tif stateValues[minCountAction]['count'] > stateValues[action]['count']: # see which one is the lesser explored action\n",
    "# \t\t\tminCountAction = action # choose the action that was explored less\n",
    "# \t# Computing the decay of the exploration\n",
    "# \tif randint(0, 100) < decay: # depending on the decay value, btw the decay decreases with more episodes, you choose to explore or go with learning\n",
    "# \t\treturn minCountAction\n",
    "# \telse:\n",
    "# \t\treturn maxValueAction\n",
    "\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "env.seed(123)\n",
    "nbEpisodes = 1\n",
    "stepsHistory = [0]*nbEpisodes\n",
    "\n",
    "history = {}  # 'state' ==> [{'count': int, 'value': float}]\n",
    "policy = defaultdict(int) \n",
    "returns_sum = defaultdict(float)\n",
    "returns_count = defaultdict(float)\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "learn(nbEpisodes)\n",
    "\n",
    "env.close()\n",
    "plt.plot(range(nbEpisodes), stepsHistory, range(nbEpisodes), [195]*nbEpisodes)\n",
    "plt.ylabel('Number of steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eea033c3-175f-4356-af15-3d570659de56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x00000186AF97C430>,\n",
      "            {   0: array([-4.79726963e+08, -2.14156179e+06, -1.46228314e+53, -5.60237368e+05]),\n",
      "                1: array([ 0.00000000e+00, -1.10027847e+22,  0.00000000e+00,  0.00000000e+00]),\n",
      "                2: array([ 0.00000000e+00,  0.00000000e+00, -2.10994378e+25,  0.00000000e+00]),\n",
      "                3: array([-3.09401619e+15,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]),\n",
      "                4: array([ 0.00000000e+00,  0.00000000e+00, -8.63817467e+22,  0.00000000e+00]),\n",
      "                6: array([        0.       ,         0.       ,         0.       ,\n",
      "       -16264038.1248231]),\n",
      "                8: array([       0.        , -3968653.33308082,        0.        ,\n",
      "              0.        ]),\n",
      "                9: array([    0.        ,     0.        ,     0.        , -1915.27734352]),\n",
      "                10: array([ 0.   ,  0.   , -1.325,  0.   ]),\n",
      "                14: array([0.        , 1.16666667, 0.        , 0.        ])})\n",
      "defaultdict(<function <lambda> at 0x00000186AF900820>,\n",
      "            {   0: 2,\n",
      "                1: 1,\n",
      "                2: 2,\n",
      "                3: 0,\n",
      "                4: 2,\n",
      "                6: 3,\n",
      "                8: 1,\n",
      "                9: 3,\n",
      "                10: 2,\n",
      "                14: 1})\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from random import randint\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict# initiatlize:\n",
    "# policy\n",
    "policy = defaultdict(lambda: randint(0,3)) # initialize random actions to the start initital policy, lambda: 0\n",
    "state_action_returns = defaultdict(lambda: 0)\n",
    "state_action_count = defaultdict(lambda: 0) # since later we need the average, we need to make sure that we know how many times the state was visited, so as not to overbias with it's high values\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "n_episodes = 100\n",
    "\n",
    " # Reward schedule:\n",
    " #    - Reach goal(G): +1\n",
    " #    - Reach hole(H): -1\n",
    " #    - Reach frozen(F): 0\n",
    "for i_episode in range(n_episodes):\n",
    "    curr_state =  env.reset()\n",
    "    start = True # for the starting taking a randoming action, to give us exploring starts, this so that whatever direction it takes, it can reach the goal\n",
    "    state_action_returns_episode = [] # to track the Q(s,a) in each episode\n",
    "    states_in_episode = [] # to track S in each episode\n",
    "    while(True): # proceed until you reach you end goals\n",
    "        if(start):\n",
    "            action = np.random.choice([0,1,2,3])\n",
    "            start=False\n",
    "        else:\n",
    "            action = policy[curr_state]\n",
    "        next_state, reward, end, probability = env.step(action)\n",
    "        if(next_state == 15):\n",
    "            reward = 1\n",
    "        elif(next_state in [5, 7, 11, 12]):\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 0\n",
    "        state_action_returns_episode.append(((curr_state, action),reward)) # to match our state_action_returns\n",
    "        states_in_episode.append(curr_state)\n",
    "        if end:\n",
    "            if(i_episode > 49950):\n",
    "                print(reward)\n",
    "                print(\"reached end goal in: \", len(states_in_episode))\n",
    "            break\n",
    "        curr_state = next_state\n",
    "     \n",
    "    # update the state-action pair values \n",
    "    for ((curr_state, action),reward) in state_action_returns_episode:\n",
    "        first_occurence_idx = next(i for i,(s_a,r) in enumerate(state_action_returns_episode) if s_a==(curr_state,action))\n",
    "        g = 0\n",
    "        for event in state_action_returns_episode:\n",
    "            # print(event)\n",
    "            g += event[1] # at the first round rewards will be 0 as the goal is not reached and it's still exploring\n",
    "        state_action_count[(curr_state, action)] += 1\n",
    "        if(i_episode > 49950):\n",
    "            print(state_action_returns[(curr_state, action)])\n",
    "        # new_val = (current_val * N + reward) / N + 1\n",
    "        state_action_returns[(curr_state, action)] += (state_action_returns[(curr_state, action)] * state_action_count[(curr_state, action)] + g) / (state_action_count[(curr_state, action)] + 1)\n",
    "        Q[curr_state][action] = state_action_returns[(curr_state, action)]\n",
    "        \n",
    "    for curr_state in states_in_episode:\n",
    "        # get all the curr_state values\n",
    "        curr_state_action_pairs = [(s,a) for ((s,a),r) in state_action_returns_episode if s==curr_state]\n",
    "        maximum = 0\n",
    "        for pair in curr_state_action_pairs:\n",
    "            if Q[pair[0]][pair[1]] > maximum:\n",
    "                maximum = Q[pair[0]][pair[1]]\n",
    "                policy[curr_state] = pair[1]\n",
    "                \n",
    "pp.pprint(Q)\n",
    "pp.pprint(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2640885-06c0-4343-86f9-8691f919c9c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(0, array([0., 0., 0., 0.])), (1, array([0., 0., 0., 0.])), (4, array([0., 0., 0., 0.])), (8, array([0., 0., 0., 0.])), (9, array([0., 0., 0., 0.])), (10, array([0., 0., 0., 0.])), (13, array([0., 0., 0., 0.])), (14, array([0.        , 0.00108696, 0.        , 0.        ])), (6, array([0., 0., 0., 0.]))])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [36], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     action_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(actions)\n\u001b[0;32m     43\u001b[0m     V[state] \u001b[38;5;241m=\u001b[39m action_value\n\u001b[1;32m---> 45\u001b[0m \u001b[43mplot_value_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptimal Value Function\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [36], line 9\u001b[0m, in \u001b[0;36mplot_value_function\u001b[1;34m(V, title)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mPlots the value function as a surface plot.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print([pair for k in V.keys() for pair in V[k]])\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# print([pair for k in V.keys() for pair in V[k]])\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(V)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m min_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m max_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(k[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m V\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     11\u001b[0m min_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(k[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m V\u001b[38;5;241m.\u001b[39mkeys())\n",
      "Cell \u001b[1;32mIn [36], line 9\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mPlots the value function as a surface plot.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print([pair for k in V.keys() for pair in V[k]])\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# print([pair for k in V.keys() for pair in V[k]])\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(V)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m min_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[43mk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m V\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     10\u001b[0m max_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(k[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m V\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     11\u001b[0m min_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(k[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m V\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def plot_value_function(V, title=\"Value Function\"):\n",
    "    \"\"\"\n",
    "    Plots the value function as a surface plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # print([pair for k in V.keys() for pair in V[k]])\n",
    "    # print([pair for k in V.keys() for pair in V[k]])\n",
    "    print(V)\n",
    "    min_x = min(k[0] for k in V.keys())\n",
    "    max_x = max(k[0] for k in V.keys())\n",
    "    min_y = min(k[1] for k in V.keys())\n",
    "    max_y = max(k[1] for k in V.keys())\n",
    "\n",
    "\n",
    "    x_range = np.arange(min_x, max_x + 1)\n",
    "    y_range = np.arange(min_y, max_y + 1)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "    # Find value for all (x, y) coordinates\n",
    "    Z_noace = np.apply_along_axis(lambda _: V[(_[0], _[1], False)], 2, np.dstack([X, Y]))\n",
    "    Z_ace = np.apply_along_axis(lambda _: V[(_[0], _[1], True)], 2, np.dstack([X, Y]))\n",
    "\n",
    "    def plot_surface(X, Y, Z, title):\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                               cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
    "        ax.set_xlabel('Player Sum')\n",
    "        ax.set_ylabel('Dealer Showing')\n",
    "        ax.set_zlabel('Value')\n",
    "        ax.set_title(title)\n",
    "        ax.view_init(ax.elev, -120)\n",
    "        fig.colorbar(surf)\n",
    "        plt.show()\n",
    "\n",
    "    plot_surface(X, Y, Z_noace, \"{} (No Usable Ace)\".format(title))\n",
    "    plot_surface(X, Y, Z_ace, \"{} (Usable Ace)\".format(title))\n",
    "    \n",
    "V = defaultdict(float)\n",
    "print(Q.items())\n",
    "for state, actions in Q.items():\n",
    "    action_value = np.max(actions)\n",
    "    V[state] = action_value\n",
    "\n",
    "plot_value_function(V, title=\"Optimal Value Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aea458db-7f22-4234-85b4-ac0746673ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# a, b, c, d = env.step(3)\n",
    "# print(a, b, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1006d09-8639-4390-b906-3ed3f93ccf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# DFS to check that it's a valid path.\n",
    "def is_valid(board: List[List[str]], max_size: int) -> bool:\n",
    "    frontier, discovered = [], set()\n",
    "    frontier.append((0, 0))\n",
    "    while frontier:\n",
    "        r, c = frontier.pop()\n",
    "        if not (r, c) in discovered:\n",
    "            discovered.add((r, c))\n",
    "            directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "            for x, y in directions:\n",
    "                r_new = r + x\n",
    "                c_new = c + y\n",
    "                if r_new < 0 or r_new >= max_size or c_new < 0 or c_new >= max_size:\n",
    "                    continue\n",
    "                if board[r_new][c_new] == \"G\":\n",
    "                    return True\n",
    "                if board[r_new][c_new] != \"H\":\n",
    "                    frontier.append((r_new, c_new))\n",
    "    return False\n",
    "def generate_random_map(size: int = 8, p: float = 0.8) -> List[str]:\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    Args:\n",
    "        size: size of each side of the grid\n",
    "        p: probability that a tile is frozen\n",
    "    Returns:\n",
    "        A random valid map\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "    board = []  # initialize to make pyright happy\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        board = np.random.choice([\"F\", \"H\"], (size, size), p=[p, 1 - p])\n",
    "        board[0][0] = \"S\"\n",
    "        board[-1][-1] = \"G\"\n",
    "        valid = is_valid(board, size)\n",
    "    return [\"\".join(x) for x in board]\n",
    "pp = pprint.PrettyPrinter(indent=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad86b075-1a9d-4ddc-a2b5-c05d536a6807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# sarsa implementation\n",
    "import gym\n",
    "from random import randint\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict # initiatlize:\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "env.render()\n",
    "\n",
    "# check to see if you can tune these values and how to tune them\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "discount_rate = 0.9\n",
    "\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "# print(env.observation_space.n)\n",
    "policy = defaultdict(lambda: 0)\n",
    "state = defaultdict(lambda: 0)\n",
    "\n",
    "n_episodes = 100\n",
    "\n",
    "max_steps = 100\n",
    "\n",
    "def choose_action_sarsa(state):\n",
    "    action = 0 # initiate arbitary action\n",
    "    p = np.random.random()\n",
    "    if p < epsilon:\n",
    "        # print('ep')\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        # print(\"max\")\n",
    "        action = np.argmax(Q[(state, action)])\n",
    "        # action = np.argmax((st, act) for )\n",
    "    return action\n",
    "\n",
    "for i_episode in range(n_episodes):\n",
    "    state =  env.reset()\n",
    "    # if the action state action does not exist, we create it, else use the existing ones\n",
    "    # if state not in policy:\n",
    "    # for a in range(env.action_space.n):\n",
    "    #         Q[(state, a)]\n",
    "    action = choose_action_sarsa(state)\n",
    "    count = 0\n",
    "    while(True):\n",
    "        # take action, observe reward and next state\n",
    "        next_state, reward, end, probability = env.step(action)\n",
    "        # choose the action for the next state as well using the policy from Q\n",
    "        next_state_action = choose_action_sarsa(next_state)\n",
    "        if(next_state == 15):\n",
    "            reward = 1\n",
    "        elif(next_state in [5, 7, 11, 12]):\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 0\n",
    "        Q[(state, action)] = Q[(state, action)] + alpha * (reward + discount_rate * Q[(next_state, next_state_action)] - Q[(state, action)])\n",
    "        count += 1\n",
    "        if end or (count > max_steps):\n",
    "            # print(\"Reached goal in steps: \", count)\n",
    "            break\n",
    "        state = next_state\n",
    "        action = next_state_action\n",
    "    \n",
    "    # print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf8c70cf-ad5c-4a6f-a2ea-fc0ea7f1f7be",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "defaultdict(<function <lambda> at 0x000001E5A2842A60>,\n",
      "            {         (0, 0): array([0., 0., 0., 0.]),\n",
      "                      (0, 1): array([0., 0., 0., 0.]),\n",
      "                      (0, 2): array([0., 0., 0., 0.]),\n",
      "                      (0, 3): array([0., 0., 0., 0.]),\n",
      "                      (1, 0): array([-0.19285354, -0.19285354, -0.19285354, -0.19285354]),\n",
      "                      (1, 1): array([-0.27977869, -0.27977869, -0.27977869, -0.27977869]),\n",
      "                      (1, 2): array([0., 0., 0., 0.]),\n",
      "                      (1, 3): array([0., 0., 0., 0.]),\n",
      "                      (2, 0): array([0., 0., 0., 0.]),\n",
      "                      (2, 1): array([0., 0., 0., 0.]),\n",
      "                      (2, 2): array([0., 0., 0., 0.]),\n",
      "                      (2, 3): array([0., 0., 0., 0.]),\n",
      "                      (4, 0): array([0., 0., 0., 0.]),\n",
      "                      (4, 1): array([-0.22048433, -0.22048433, -0.22048433, -0.22048433]),\n",
      "                      (4, 2): array([-0.31310148, -0.31310148, -0.31310148, -0.31310148]),\n",
      "                      (4, 3): array([-0.07361221, -0.07361221, -0.07361221, -0.07361221]),\n",
      "                      (5, 0): array([0., 0., 0., 0.]),\n",
      "                      (5, 1): array([0., 0., 0., 0.]),\n",
      "                      (5, 2): array([0., 0., 0., 0.]),\n",
      "                      (5, 3): array([0., 0., 0., 0.]),\n",
      "                      (6, 0): array([-0.1629, -0.1629, -0.1629, -0.1629]),\n",
      "                      (8, 0): array([-0.25072083, -0.25072083, -0.25072083, -0.25072083]),\n",
      "                      (8, 1): array([-0.58756551, -0.58756551, -0.58756551, -0.58756551]),\n",
      "                      (8, 2): array([-0.34857694, -0.34857694, -0.34857694, -0.34857694]),\n",
      "                      (8, 3): array([0., 0., 0., 0.]),\n",
      "                      (9, 0): array([-0.29470468, -0.29470468, -0.29470468, -0.29470468]),\n",
      "                      (9, 1): array([0., 0., 0., 0.]),\n",
      "                      (9, 2): array([0., 0., 0., 0.]),\n",
      "                      (9, 3): array([0., 0., 0., 0.]),\n",
      "                      (10, 0): array([0., 0., 0., 0.]),\n",
      "                      (10, 1): array([0., 0., 0., 0.]),\n",
      "                      (12, 0): array([0., 0., 0., 0.]),\n",
      "                      (12, 1): array([0., 0., 0., 0.]),\n",
      "                      (12, 2): array([0., 0., 0., 0.]),\n",
      "                      (13, 0): array([-0.46593126, -0.46593126, -0.46593126, -0.46593126]),\n",
      "                      (13, 1): array([-0.1, -0.1, -0.1, -0.1]),\n",
      "                      (13, 2): array([0., 0., 0., 0.]),\n",
      "                      (13, 3): array([0., 0., 0., 0.]),\n",
      "                      (14, 0): array([0., 0., 0., 0.])})\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning implementation\n",
    "import gym\n",
    "from random import randint\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict # initiatlize:\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "env.render()\n",
    "\n",
    "# check to see if you can tune these values and how to tune them\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "discount_rate = 0.9\n",
    "\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "# print(env.observation_space.n)\n",
    "policy = defaultdict(lambda: 0)\n",
    "state = defaultdict(lambda: 0)\n",
    "\n",
    "def choose_action_q_learning(state):\n",
    "    action = 0 # initiate arbitary action\n",
    "    p = np.random.random()\n",
    "    if p < epsilon:\n",
    "        # print('ep')\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        # print(\"max\")\n",
    "        action = np.argmax(Q[(state, action)])\n",
    "        # action = np.argmax((st, act) for )\n",
    "    return action\n",
    "\n",
    "n_episodes = 1000\n",
    "\n",
    "max_steps = 200\n",
    "\n",
    "for i_episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    count = 0\n",
    "    while(True):\n",
    "        # choose A from S using policy derived from Q\n",
    "        action =  choose_action_q_learning(state)\n",
    "        # take action A, observe reward and next state\n",
    "        next_state, reward, end, probability = env.step(action)\n",
    "        count += 1\n",
    "        if(next_state == 15):\n",
    "            reward = 1\n",
    "        elif(next_state in [5, 7, 11, 12]):\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 0\n",
    "        Q[(state, action)] = Q[(state, action)] + alpha * (reward + discount_rate * np.argmax(Q[(next_state, action)]) - Q[(state, action)])\n",
    "        if end or (count > max_steps):\n",
    "            # print(\"Reaching steps in: \", count)\n",
    "            break\n",
    "        state = next_state\n",
    "        \n",
    "        \n",
    "pp.pprint(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94892b1-955d-40b6-867e-57ee61d01013",
   "metadata": {},
   "source": [
    "10 x 10 version from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e293a470-2fb6-4ce0-b260-69a3e529e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from random import randint\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict # initiatlize:\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=generate_random_map(size=10))\n",
    "env.render()\n",
    "# policy\n",
    "policy = defaultdict(lambda: randint(0,3)) # initialize random actions to the start initital policy, lambda: 0\n",
    "state_action_returns = defaultdict(lambda: 0)\n",
    "state_action_count = defaultdict(lambda: 0) # since later we need the average, we need to make sure that we know how many times the state was visited, so as not to overbias with it's high values\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "n_episodes = 50000\n",
    "\n",
    " # Reward schedule:\n",
    " #    - Reach goal(G): +1\n",
    " #    - Reach hole(H): -1\n",
    " #    - Reach frozen(F): 0\n",
    "for i_episode in range(n_episodes):\n",
    "    curr_state =  env.reset()\n",
    "    start = True # for the starting taking a randoming action, to give us exploring starts, this so that whatever direction it takes, it can reach the goal\n",
    "    state_action_returns_episode = [] # to track the Q(s,a) in each episode\n",
    "    states_in_episode = [] # to track S in each episode\n",
    "    while(True): # proceed until you reach you end goals\n",
    "        if(start):\n",
    "            action = np.random.choice([0,1,2,3])\n",
    "            start=False\n",
    "        else:\n",
    "            action = policy[curr_state]\n",
    "        next_state, reward, end, probability = env.step(action)\n",
    "        if(next_state == 15):\n",
    "            reward = 1\n",
    "        elif(next_state in [5, 7, 11, 12]):\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 0\n",
    "        state_action_returns_episode.append(((curr_state, action),reward)) # to match our state_action_returns\n",
    "        states_in_episode.append(curr_state)\n",
    "        if end:\n",
    "            if(i_episode > 49998):\n",
    "                print(reward)\n",
    "                print(\"reached end goal in: \", len(states_in_episode))\n",
    "            break\n",
    "        curr_state = next_state\n",
    "     \n",
    "    # update the state-action pair values \n",
    "    for ((curr_state, action),reward) in state_action_returns_episode:\n",
    "        first_occurence_idx = next(i for i,(s_a,r) in enumerate(state_action_returns_episode) if s_a==(curr_state,action))\n",
    "        g = 0\n",
    "        for event in state_action_returns_episode:\n",
    "            # print(event)\n",
    "            g += event[1] # at the first round rewards will be 0 as the goal is not reached and it's still exploring\n",
    "        state_action_count[(curr_state, action)] += 1\n",
    "        if(i_episode > 49998):\n",
    "            print(state_action_returns[(curr_state, action)])\n",
    "        # new_val = (current_val * N + reward) / N + 1\n",
    "        state_action_returns[(curr_state, action)] += (state_action_returns[(curr_state, action)] * state_action_count[(curr_state, action)] + g) / (state_action_count[(curr_state, action)] + 1)\n",
    "        Q[curr_state][action] = state_action_returns[(curr_state, action)]\n",
    "        \n",
    "    for curr_state in states_in_episode:\n",
    "        # get all the curr_state values\n",
    "        curr_state_action_pairs = [(s,a) for ((s,a),r) in state_action_returns_episode if s==curr_state]\n",
    "        maximum = 0\n",
    "        for pair in curr_state_action_pairs:\n",
    "            if Q[pair[0]][pair[1]] > maximum:\n",
    "                maximum = Q[pair[0]][pair[1]]\n",
    "                policy[curr_state] = pair[1]\n",
    "                \n",
    "pp.pprint(Q)\n",
    "pp.pprint(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf1713-3e34-4607-bd64-f66320475eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a104a05-010d-44e3-a9e2-1b89a88e5a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
