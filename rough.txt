# Q-learning
import gym
from random import randint
import math
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict # initiatlize:

env = gym.make('FrozenLake-v1', desc=generate_random_map(size=10, p = 0.75))
env.render()

# check to see if you can tune these values and how to tune them
alpha = 0.4
# epsilon = 0.2
discount_rate = 1

Q_list = []
# Q = defaultdict(lambda: np.zeros(env.action_space.n))
# print(env.observation_space.n)
policy = defaultdict(lambda: 0)
state = defaultdict(lambda: 0)

def choose_action_q_learning(Q, state):
    max_action = env.action_space.sample()
    min_action = env.action_space.sample() # initiate arbitary action
    # p = np.random.random()
    
    for action in range(env.action_space.n):
        if Q[(state, max_action)]["a"] < Q[(state, action)]["a"]:
            max_action = action
        if Q[(state, max_action)]["c"] < Q[(state, action)]["c"]:
            min_action = action
    # if p < epsilon:
    #     # print('ep')
    #     action = env.action_space.sample()
    # else:
    #     # print("max")
    #     action = np.argmax(Q[(state, action)])
        # action = np.argmax((st, act) for )
    # return action
    if randint(0, 100) < decay:
        return min_action # lesser explored
    else:
        return max_action # exploitation

# def choose_action_q_learning(state):
#     action = 0 # initiate arbitary action
#     p = np.random.random()
#     if p < epsilon:
#         # print('ep')
#         action = env.action_space.sample()
#     else:
#         # print("max")
#         action = np.argmax(Q[(state, action)])
#         # action = np.argmax((st, act) for )
#     return action

n_episodes = 100000

max_steps = 200

steps_per_episode = []
steps_per_episode_goal = []
size = int(math.sqrt(env.observation_space.n))

for epsilon in np.arange(0, 1, 0.1):
    
    print(epsilon)
    
    
    # Q = defaultdict(lambda: np.zeros(env.action_space.n))
    # Q = defaultdict(lambda: 0)
    Q = defaultdict(lambda: {"a": 0, "c": 0}) # action value and the count

    for i_episode in range(n_episodes):
        decayX = 0.02
        decayY = 20
        decay = max(-i_episode*decayX+decayY, 10/(i_episode+1))

        state = env.reset()
        count = 0
        while(True):
            # choose A from S using policy derived from Q
            action =  choose_action_q_learning(Q, state)
            # take action A, observe reward and next state
            next_state, reward, end, probability = env.step(action)
            count += 1 # steps
            if(env.desc[next_state//size][next_state%size] == b"G"):
                # print(len(steps_per_episode_goal))
                reward = 1
            elif(env.desc[next_state//size][next_state%size] == b"H"): # need to add the holes
                reward = -1
            else:
                reward = 0
            Q[(state, action)]["a"] = Q[(state, action)]["a"] + alpha * (reward + discount_rate * np.argmax(Q[(state, action)]["a"] - Q[(state, action)]["a"]))
            Q[(state, action)]["c"] += 1 # number of time the state action was visited                         
            if end: # don't include max_steps first
                # print("Reaching steps in: ", count)
                if(env.desc[next_state//size][next_state%size] == b"G"):
                    steps_per_episode_goal.append((i_episode, count))
                    # print("goal")
                else:
                    steps_per_episode.append((i_episode, count))
                break
            state = next_state
        if(i_episode % 10000 == 0):
                print("Per episode", i_episode)
                
    Q_list.append(Q) # get the policies for comparison
        
        
# pp.pprint(Q)